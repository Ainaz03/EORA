# README — Тестовое задание EORA

Проект: EORA LLM Assistant — прототип сервиса, который отвечает на вопросы потенциальных клиентов, учитывая материалы с сайта eora.ru.

Реализованный уровень: Средний. Ответ формируется LLM-ом (или мок-LLM), а сервис возвращает список используемых источников и фрагменты, на которых опирался.

## Что реализовано (функциональность)

### HTTP API (FastAPI) с endpoint /query, принимающим:

question — текст вопроса,

urls — опциональный список URL для поиска (если не передан — берутся app/sources.txt),

top_k — сколько отрывков подставлять в контекст.

### Скрейпер: скачивание страниц и кеширование текста (data/cache/) с очисткой HTML.

### Retriever: разбиение текстов на отрывки (параграфы), TF-IDF векторизация (scikit-learn) и поиск самых похожих отрывков по cosine similarity (возврат top-k).

### LLM client:

Mock-режим (по умолчанию) — позволяет запускать весь пайплайн без API-ключей;

Заготовка для подключения OpenAI (или другой облачной LLM) — переключается переменной окружения.

## Возвращаемая структура ответа:

{
  "answer": "...",
  "sources": ["https://...","..."],
  "used_passages": ["текст фрагмента 1", "фрагмент 2"]
}

## Как это работает (архитектура и ключевые файлы)

main.py вызывает ensure_cached_texts(urls) → retriever.retrieve(question, urls, top_k) → llm.ask_with_context(question, top_docs) → ответ клиенту.

Retriever грузит кешированные тексты (или вызывает скрейпер для кеша), разбивает на параграфы (passages), строит TF-IDF матрицу и вычисляет косинусное сходство с вектором вопроса. Берёт top-k наиболее релевантных отрывков.

LLM получает вопрос + отрывки и формирует ответ. В mock-режиме возвращается простая заглушка — это позволяет тестировать всё без API-ключей.

## Быстрая инструкция по запуску (локально)
1) Клонировать или подготовить проект
```
git clone https://github.com/Ainaz03/EORA
cd eora
```
2) Виртуальное окружение и зависимости
```
python -m venv .venv
source .venv/bin/activate      # Linux / macOS
# .venv\Scripts\activate       # Windows PowerShell
pip install -r requirements.txt
```

requirements.txt минимальный:

fastapi
uvicorn
requests
beautifulsoup4
scikit-learn
python-dotenv

3) Переменные окружения
```
MOCK_MODE=true
OPENAI_API_KEY=
CACHE_DIR=./data/cache
```

MOCK_MODE=true — работает без API ключа (рекомендуется для проверки).

Чтобы подключить реальное OpenAI: MOCK_MODE=false и OPENAI_API_KEY=sk-...

4) Запуск сервера
```
uvicorn app.main:app --reload --port 8000
```
6) Пример запроса
```
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"question":"Что вы можете сделать для ритейлеров?","top_k":3}'
```

Ответ — JSON с полями answer, sources, used_passages. В mock-режиме answer будет помечен как mock.

5. Примеры использования / формат ответа

Вопрос:

Что вы можете сделать для ритейлеров?

Пример ответа (mock):

[Mock answer] Вопрос: Что вы можете сделать для ритейлеров? — используется 3 источников.


Пример ответа (реальная LLM):

Мы помогаем ритейлерам автоматизировать контакт-центры и улучшать поиск товаров по фото. Например, мы реализовывали HR-бота для Магнита [1] и поиск по картинкам для KazanExpress [2].

Источники:
[1] https://eora.ru/cases/chat-boty/hr-bot-dlya-magnit-kotoriy-priglashaet-na-sobesedovanie
[2] https://eora.ru/cases/kazanexpress-poisk-tovarov-po-foto


(в этой реализации ссылки возвращаются в массиве sources и фрагменты — в used_passages; вставки [1] в теле ответа возможны при доработке prompt-а / post-processing)

## Что пробовал сделать — итерации разработки

Сделан минимальный, но полный pipeline: скрейпинг → кеш → разбивка на отрывки → TF-IDF → выбор top-k → подстановка в prompt LLM.

Проверка работы пайплайна в mock-режиме: можно тестировать сбор и релевантность отрывков без затрат.

Заготовка для интеграции с OpenAI (ChatCompletion) — переключается через переменные окружения.

## Что сработало, а что не очень

Сработало:

Надёжный базовый скрейпер (BeautifulSoup) + кеширование — быстро повторно использует уже скачанные тексты.

TF-IDF + cosine similarity даёт быстрые и интерпретируемые результаты для подборки релевантных отрывков.

Mock-режим позволяет полностью тестировать систему без ключей.

## Ограничения / что не очень:

TF-IDF — линеен по «поверхностному» совпадению терминов; семантические совпадения лучше ловят эмбеддинги.

При больших объёмах данных и длинных страниц нужны: chunking по токенам, бюджет токенов для LLM и ANN индекс (FAISS/HNSW) для скорости.

В текущей реализации ссылки не вставляются автоматически inline в тексте ответа — только возвращаются в sources. Сложный уровень требует дополнительной логики в prompt и/или пост-обработке.

## Что можно добавить при наличии времени (планы улучшений)

Перейти на эмбеддинги (OpenAI embeddings / local) + ANN (FAISS / hnswlib) для лучшей семантики и ускорения поиска.

Реализовать сложный уровень: LLM генерирует текст с inline-ссылками. Подход: в prompt просить LLM помечать использованные фрагменты маркерами [n], затем собрать n → URL карту.

Ограничение контекста по токенам и интеллектуальный chunking (rank-and-trim before sending to LLM).

UI: чат-виджет (React) или Telegram-бот — для демонстрации внешнего интерфейса.

Поддержка PDF / DOCX / изображений (OCR + извлечение текста) в индекс.

Аутентификация, rate-limit, логирование, мониторинг и CI (тесты).

Тестирование безопасности (валидация URL, защита от SSRF, timeouts).
